{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c2508d",
   "metadata": {},
   "source": [
    "## Model Fitting\n",
    "\n",
    "Lets fit the Logestic Regression Model, Support vector Machines and the Decision Tree models on this data set and do the analysis and get the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f79772",
   "metadata": {},
   "source": [
    "### Importing the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9026ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "\n",
    "# setting random seed to ensure that results are repeatable\n",
    "np.random.seed(7026)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac44028",
   "metadata": {},
   "source": [
    "### Importing the train and test data set from the Data processing notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01cda7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.read_csv(\"smoke_train_X.csv\")\n",
    "X_test=pd.read_csv(\"smoke_test_X.csv\")\n",
    "y_train=pd.read_csv(\"smoke_train_y.csv\")\n",
    "y_test=pd.read_csv(\"smoke_test_y.csv\")\n",
    "\n",
    "train_df=pd.read_csv(\"smoke_train_df.csv\")\n",
    "test_df=pd.read_csv(\"smoke_test_y.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d71d1",
   "metadata": {},
   "source": [
    "### Standardizing the variables\n",
    "\n",
    "We standardize our variables to eliminate the differences in scale between the variables/attributes.\n",
    "\n",
    "We will use the sklearn library's 'standard scaler' to accomplish this. The standard scaler function will standardize our variables. To achieve this, we will first need to train the scaler on the training data and then apply this trained scaler to standardize both the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49e8dc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform the predictors of training and test sets\n",
    "X_train = scaler.transform(X_train) \n",
    " \n",
    "\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1474d59",
   "metadata": {},
   "source": [
    "### Checking for the Imbalance in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8263ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    31421\n",
       "0    12420\n",
       "Name: Fire_Alarm, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Fire_Alarm.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422e9513",
   "metadata": {},
   "source": [
    "We can clearlly observe the data imbalance in this data so now lets do the undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aadb638",
   "metadata": {},
   "source": [
    "### Undersampling the data to get the balace in the data\n",
    "\n",
    "The reason for undersampling is that there are more observations in the data. So, undersampling th data can acheive the data balance and also helps us remove extra data\n",
    "\n",
    "Lets use the random under sampler to undersample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "145dd7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "undersample = RandomUnderSampler(sampling_strategy='majority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b363e8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train=undersample.fit_resample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1795c59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fire_Alarm\n",
       "0             12420\n",
       "1             12420\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538b4500",
   "metadata": {},
   "source": [
    "Now the data is balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df632f71",
   "metadata": {},
   "source": [
    "### Deciding on the best evalution metrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bf2c55",
   "metadata": {},
   "source": [
    "Our main aim is to reduce the False Alarms which are nothing but the False Positives(Detected the smoke but there is actualy no smoke). But neglectng the False Negatives is even more dangerous as it doesn't detect smoke but there is actually smoke which could potentially be a fire.\n",
    "\n",
    "Which means we have deal with both False Negatives and False Positives and the best evalution metric for this is **'F1 SCORE'**.\n",
    "F1 score is the harmonic mean of Recall and precision so it deals with both false negatives and false postives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b7a7a0",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0681dc93",
   "metadata": {},
   "source": [
    "Lets Create a data frame to store the results of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dc20b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame({\"model\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83622125",
   "metadata": {},
   "source": [
    "### Logistic Regression Model\n",
    "\n",
    "#### Logistic Regression model using Random Search and Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cbe109b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "1020 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "355 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1048, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 864, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 782, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "335 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 71, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "330 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.90665343 0.90669782 0.90613967 0.89017644 0.90666112\n",
      " 0.90654873        nan        nan 0.8995133  0.8452965         nan\n",
      " 0.9064924  0.90653543 0.89017644        nan        nan 0.85252436\n",
      "        nan        nan 0.90641564 0.90665343        nan 0.89017644\n",
      "        nan 0.8452965         nan        nan 0.89951378        nan\n",
      " 0.90642329 0.90652719        nan 0.8995133  0.90613967 0.90665343\n",
      "        nan 0.90623339        nan 0.90666033 0.8452965  0.81236847\n",
      "        nan 0.90593584 0.90644823        nan        nan        nan\n",
      " 0.8995133  0.89951378 0.90654338 0.90660876 0.89017644 0.90616634\n",
      "        nan 0.9060427  0.90613967        nan 0.9062644  0.90660027\n",
      "        nan 0.9060427         nan 0.90640157        nan        nan\n",
      " 0.85252436 0.90660027        nan        nan 0.89951378        nan\n",
      " 0.89951378 0.90630613 0.90598021 0.90598724        nan 0.9063798\n",
      "        nan 0.90638612 0.90652846        nan 0.85252436        nan\n",
      "        nan 0.90642308 0.89263545        nan 0.90669682 0.906006\n",
      "        nan        nan        nan 0.9064553         nan 0.90660794\n",
      "        nan 0.90597475 0.906006   0.8452965         nan 0.89207672\n",
      "        nan 0.90630613 0.90666033 0.81754247 0.89159831 0.81236847\n",
      "        nan 0.90649899 0.81754247 0.89159831 0.89017644 0.90645229\n",
      "        nan        nan        nan        nan        nan 0.85252436\n",
      " 0.90657236        nan        nan        nan        nan        nan\n",
      " 0.89951378 0.90642249 0.90642249 0.89951378 0.90629983        nan\n",
      "        nan        nan 0.89263545        nan 0.9063798         nan\n",
      " 0.90648491 0.9064553         nan        nan        nan        nan\n",
      " 0.90666112        nan 0.8995133  0.9063798  0.90657943 0.89017644\n",
      "        nan 0.89263545 0.90605648 0.90656546 0.9064553         nan\n",
      "        nan        nan        nan        nan 0.90652719 0.90609253\n",
      " 0.90612293 0.8995133         nan 0.81236847        nan        nan\n",
      " 0.90649898        nan 0.81757657        nan 0.90654873        nan\n",
      " 0.90661614        nan 0.90657154 0.90630814 0.89951378 0.90623339\n",
      " 0.89263545 0.90666112 0.81757657        nan 0.90657236        nan\n",
      " 0.90666033 0.89951378        nan 0.8995133         nan        nan\n",
      " 0.85252436        nan 0.9064188         nan 0.8452965  0.85252436\n",
      " 0.8452965  0.8452965         nan        nan        nan        nan\n",
      "        nan        nan 0.90669782        nan        nan 0.89263545\n",
      "        nan        nan        nan 0.9060173         nan        nan\n",
      "        nan 0.90653543        nan 0.89017644        nan        nan\n",
      "        nan        nan 0.8452965  0.89159831        nan 0.89951378\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.89951378 0.81754247 0.90665343 0.81236847 0.89159831        nan\n",
      " 0.81236847        nan        nan 0.90627003 0.89951378 0.85252436\n",
      "        nan        nan 0.90612937 0.90644823 0.90642565 0.89951378\n",
      " 0.90653543 0.90642249 0.90652719        nan 0.90619655 0.90644796\n",
      " 0.89951378 0.90657943 0.89159831        nan 0.81236847 0.90598021\n",
      " 0.90643802        nan 0.81236847 0.81236847 0.90669782 0.90661695\n",
      "        nan 0.90630105 0.81236847        nan 0.90666033        nan\n",
      "        nan 0.90586442        nan 0.90630814 0.90651172 0.85252436\n",
      " 0.85252436        nan 0.90665343        nan 0.90649948 0.90638591\n",
      " 0.81236847        nan        nan        nan 0.89951378 0.90660027\n",
      " 0.90643066 0.90634458 0.90657189        nan 0.90649203 0.90613967\n",
      " 0.9062644         nan        nan 0.89207672 0.90666033 0.81236847\n",
      " 0.89263545 0.90646205 0.89017644 0.9063798         nan        nan\n",
      " 0.9060173  0.90619693        nan 0.89263545 0.90613967        nan\n",
      "        nan 0.9060427         nan 0.89263545        nan 0.90593634\n",
      " 0.90602434 0.90644823 0.90622644 0.90613967 0.90659971 0.90649898\n",
      "        nan        nan        nan        nan        nan 0.81236847\n",
      " 0.90654314 0.90644823 0.90597447 0.90649898        nan 0.89017644\n",
      " 0.81754247 0.90658025 0.8995133         nan        nan        nan\n",
      " 0.89263545 0.89159831        nan 0.81236847 0.906006   0.90613967\n",
      "        nan 0.90629268 0.90644823        nan        nan 0.90653621\n",
      " 0.90651172 0.90597294 0.81757657        nan 0.90613006 0.85252436\n",
      " 0.81236847        nan 0.90637912 0.90660027        nan 0.90616634\n",
      "        nan 0.90660834        nan 0.89263545 0.89951378        nan\n",
      " 0.90616567        nan 0.81233854 0.90653591        nan 0.90654273\n",
      " 0.906006   0.90664502        nan        nan        nan 0.89207672\n",
      " 0.89207672 0.89207672 0.89951378        nan 0.85252436        nan\n",
      " 0.8995133         nan 0.81757657 0.90652719        nan        nan\n",
      "        nan        nan 0.89017644        nan        nan 0.90644823\n",
      " 0.90605559 0.90661572 0.90649898        nan 0.89017644        nan\n",
      " 0.90657189 0.90660027 0.90653543 0.906006   0.9062562  0.90653543\n",
      "        nan 0.90630702        nan        nan 0.90646766        nan\n",
      "        nan        nan        nan        nan 0.90653543 0.9063798\n",
      " 0.89017644 0.90650668 0.89017644 0.8995133  0.90605552 0.90627852\n",
      "        nan 0.90664493        nan 0.90656359 0.90598021        nan\n",
      " 0.89017644        nan 0.90634507 0.90638533        nan 0.8995133\n",
      "        nan 0.90656416 0.90666071 0.90634953 0.90657154        nan\n",
      "        nan        nan 0.90662361 0.89159831        nan 0.89263545\n",
      " 0.8452965         nan        nan        nan 0.90657189 0.9063798\n",
      "        nan 0.90619693        nan        nan        nan        nan\n",
      " 0.90656416 0.90641578 0.906006   0.81754247 0.90669782 0.9063798\n",
      " 0.90613967 0.89017644 0.90630702 0.90646727 0.81236847        nan\n",
      "        nan 0.906006          nan 0.90669682        nan 0.90660066\n",
      "        nan 0.90595065        nan 0.9060173  0.90653591 0.9060427\n",
      " 0.9060173  0.90613967]\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.9066978169406108\n",
      "... with parameters: {'solver': 'saga', 'penalty': 'l2', 'max_iter': 785, 'C': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {'C':[0.001,0.01,0.1,1,10], # C is the regulization strength\n",
    "               'penalty':['l1', 'l2','elasticnet','none'],\n",
    "              'solver':['saga','liblinear'],\n",
    "              'max_iter': np.arange(500,1000)\n",
    "                  \n",
    "}\n",
    "\n",
    "lg = LogisticRegression()\n",
    "rand_search = RandomizedSearchCV(estimator =lg, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1  # n_jobs=-1 will utilize all available CPUs \n",
    "                                )\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestlogestic = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f7319bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 600 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.90672653110977\n",
      "... with parameters: {'C': 9.95, 'max_iter': 722, 'penalty': 'l2', 'solver': 'saga'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 5\n",
    "best_penality = rand_search.best_params_['penalty']\n",
    "best_solver = rand_search.best_params_['solver']\n",
    "min_regulization_strength=rand_search.best_params_['C']\n",
    "min_iter = rand_search.best_params_['max_iter']\n",
    "\n",
    "#Using the best parameters from the Random Search to use as range for the parameters to do the grid search\n",
    "param_grid = {\n",
    "    \n",
    "    'C':np.arange(min_regulization_strength-0.05,min_regulization_strength+0.05), \n",
    "               'penalty':[best_penality],\n",
    "              'solver':[best_solver],\n",
    "              'max_iter': np.arange(min_iter-300,min_iter+300)\n",
    "}\n",
    "\n",
    "lgr =  LogisticRegression()\n",
    "grid_search = GridSearchCV(estimator = lgr, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1 # n_jobs=-1 will utilize all available CPUs \n",
    "                )\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestlgr = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85382c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.9052637 Precision=0.9638728 Recall=0.9002699 F1=0.9309864\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")\n",
    "F1_lr=2*TP/(2*TP+FP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f844e46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score from the Logestic Regression model using Random Search and Grid Search is :0.9309863523573201\n"
     ]
    }
   ],
   "source": [
    "print(f\"The F1 score from the Logestic Regression model using Random Search and Grid Search is :{F1_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db571ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.concat([performance, pd.DataFrame({'model':\"logistic using random & grid search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e519f31",
   "metadata": {},
   "source": [
    "### SVM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed467f1",
   "metadata": {},
   "source": [
    "#### SVM using RandomSearch and Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "917d045a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 500 candidates, totalling 1500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.9995170442896705\n",
      "... with parameters: {'kernel': 'poly', 'gamma': 'auto', 'degree': 3, 'coef0': 8, 'C': 90.1}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 3\n",
    "\n",
    "param_grid = {'C':np.arange(0.1,100,10),  #  regularization parameter.\n",
    "               'kernel':['linear', 'rbf','poly'],\n",
    "              'gamma':['scale','auto'],\n",
    "              'degree':np.arange(1,10), #degree is for the polynomial kernal\n",
    "              'coef0':np.arange(1,10) #coef0 is for the polynomial kernal\n",
    "                  \n",
    "}\n",
    "\n",
    "svc = SVC()\n",
    "rand_search = RandomizedSearchCV(estimator =svc, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1  # n_jobs=-1 will utilize all available CPUs \n",
    "                                )\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestsvc = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61dc06bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.9995170442896705\n",
      "... with parameters: {'C': 87.1, 'coef0': 8, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 3\n",
    "best_kernel = rand_search.best_params_['kernel']\n",
    "best_gamma = rand_search.best_params_['gamma']\n",
    "min_regulization=rand_search.best_params_['C']\n",
    "best_degree = rand_search.best_params_['degree']\n",
    "best_coef0=rand_search.best_params_['coef0']\n",
    "\n",
    "#Using the best parameters from the Random Search to use as range for the parameters to do the grid search\n",
    "param_grid = {\n",
    "    \n",
    "    'C':np.arange(min_regulization-3,min_regulization+3), \n",
    "               'kernel':[best_kernel],\n",
    "              'gamma':[best_gamma],\n",
    "              'degree': np.arange(best_degree-1,best_degree+1),\n",
    "            'coef0': np.arange(best_coef0-3,best_coef0+3)\n",
    "}\n",
    "\n",
    "svm_grid =  SVC()\n",
    "grid_search = GridSearchCV(estimator = svm_grid, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1 # n_jobs=-1 will utilize all available CPUs \n",
    "                )\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "best_svm = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99658055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.9995210 Precision=0.9997000 Recall=0.9996251 F1=0.9996626\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")\n",
    "F1_svm=2*TP/(2*TP+FP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85adf5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 score from the SVM model using Random Search and Grid Search is 0.9996625548348393\n"
     ]
    }
   ],
   "source": [
    "print(f\"The f1 score from the SVM model using Random Search and Grid Search is {F1_svm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92570f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.concat([performance, pd.DataFrame({'model':\"svm using Random & Grid search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2104cfaa",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e53ffb",
   "metadata": {},
   "source": [
    "#### Decision Trees using RandomSearchCV combined with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9110d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best f1 score is 0.9992752810368071\n",
      "... with parameters: {'min_samples_split': 2, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.0001, 'max_leaf_nodes': 95, 'max_depth': 18, 'criterion': 'gini'}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,100),  \n",
    "    'min_samples_leaf': np.arange(1,100),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 100), \n",
    "    'max_depth': np.arange(1,25), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29c8ed75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1024 candidates, totalling 5120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "2240 fits failed out of a total of 5120.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1280 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 889, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 177, in fit\n",
      "    self._validate_params()\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 581, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_leaf' parameter of DecisionTreeClassifier must be an int in the range [1, inf) or a float in the range (0.0, 1.0). Got 0 instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "960 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 889, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 177, in fit\n",
      "    self._validate_params()\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 581, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_split' parameter of DecisionTreeClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 0 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan ... 0.99919537 0.99919537 0.99919537]\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the train scores are non-finite: [       nan        nan        nan ... 0.99955727 0.99955727 0.99955727]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.9995973910381597\n",
      "... with parameters: {'criterion': 'gini', 'max_depth': 19, 'max_leaf_nodes': 95, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 3}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 5\n",
    "min_samples_split = rand_search.best_params_['min_samples_split']\n",
    "min_samples_leaf = rand_search.best_params_['min_samples_leaf']\n",
    "min_impurity_decrease = rand_search.best_params_['min_impurity_decrease']\n",
    "max_leaf_nodes = rand_search.best_params_['max_leaf_nodes']\n",
    "max_depth = rand_search.best_params_['max_depth']\n",
    "criterion = rand_search.best_params_['criterion']\n",
    "#Using the best parameters from the Random Search to use as range for the parameters to do the grid search\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(min_samples_split-2,min_samples_split+2),  \n",
    "    'min_samples_leaf': np.arange(min_samples_leaf-2,min_samples_leaf+2),\n",
    "    'min_impurity_decrease': np.arange(min_impurity_decrease-0.0001, min_impurity_decrease+0.0001, 0.00005),\n",
    "    'max_leaf_nodes': np.arange(max_leaf_nodes-2,max_leaf_nodes+2), \n",
    "    'max_depth': np.arange(max_depth-2,max_depth+2), \n",
    "    'criterion': [criterion]\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = dtree, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d78e1c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.9997871 Precision=0.9998500 Recall=0.9998500 F1=0.9998500\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")\n",
    "F1_Decisiontree=2*TP/(2*TP+FP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27905dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 score from the Decision Tree using Random Search and Grid Search is 0.9998500299940012\n"
     ]
    }
   ],
   "source": [
    "print(f\"The f1 score from the Decision Tree using Random Search and Grid Search is {F1_Decisiontree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "042d0315",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Decision Tree\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac50f49",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d82fc99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic using random &amp; grid search</td>\n",
       "      <td>0.905264</td>\n",
       "      <td>0.963873</td>\n",
       "      <td>0.900270</td>\n",
       "      <td>0.930986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm using Random &amp; Grid search</td>\n",
       "      <td>0.999521</td>\n",
       "      <td>0.999700</td>\n",
       "      <td>0.999625</td>\n",
       "      <td>0.999663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.999787</td>\n",
       "      <td>0.999850</td>\n",
       "      <td>0.999850</td>\n",
       "      <td>0.999850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 model  Accuracy  Precision    Recall  \\\n",
       "0  logistic using random & grid search  0.905264   0.963873  0.900270   \n",
       "0       svm using Random & Grid search  0.999521   0.999700  0.999625   \n",
       "0                        Decision Tree  0.999787   0.999850  0.999850   \n",
       "\n",
       "         F1  \n",
       "0  0.930986  \n",
       "0  0.999663  \n",
       "0  0.999850  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5120c6f1",
   "metadata": {},
   "source": [
    "We can clearlly observ that the F1 score of the Decision Tree is much higher when comapred to other models. So, the best model that has less False Positives(False Alarms) and False Negatives(Smoke is there but Detected that there is no smoke) is **Decision Trees**. \n",
    "\n",
    "Although the F1 scores of Support Vector Machines and Decision Trees doesn't have much difference, the decision trees are simpler and can be tunned more such that we can acheive better results. So we can decide that the Decision Tree is the best model for this data set.\n",
    "\n",
    "Now that we have our best model we can deploye this into an AI based smoke detector and it can predict the smoke with less False Negatives and False Postives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db403e30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
