{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75595b0e",
   "metadata": {},
   "source": [
    "# WE07-Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c2508d",
   "metadata": {},
   "source": [
    "## Model Fitting\n",
    "\n",
    "Lets fit the Logestic Regression Model, Support vector Machines, Decision Tree models and Neural Networks models on this data set and do the analysis and get the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f79772",
   "metadata": {},
   "source": [
    "### Importing the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9026ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "\n",
    "# setting random seed to ensure that results are repeatable\n",
    "np.random.seed(7026)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac44028",
   "metadata": {},
   "source": [
    "### Importing the train and test data set from the Data processing notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01cda7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.read_csv(\"smoke_train_X.csv\")\n",
    "X_test=pd.read_csv(\"smoke_test_X.csv\")\n",
    "y_train=pd.read_csv(\"smoke_train_y.csv\")\n",
    "y_test=pd.read_csv(\"smoke_test_y.csv\")\n",
    "\n",
    "train_df=pd.read_csv(\"smoke_train_df.csv\")\n",
    "test_df=pd.read_csv(\"smoke_test_y.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d71d1",
   "metadata": {},
   "source": [
    "### Standardizing the variables\n",
    "\n",
    "We standardize our variables to eliminate the differences in scale between the variables/attributes.\n",
    "\n",
    "We will use the sklearn library's 'standard scaler' to accomplish this. The standard scaler function will standardize our variables. To achieve this, we will first need to train the scaler on the training data and then apply this trained scaler to standardize both the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49e8dc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform the predictors of training and test sets\n",
    "X_train = scaler.transform(X_train) \n",
    " \n",
    "\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1474d59",
   "metadata": {},
   "source": [
    "### Checking for the Imbalance in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8263ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    31421\n",
       "0    12420\n",
       "Name: Fire_Alarm, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Fire_Alarm.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422e9513",
   "metadata": {},
   "source": [
    "We can clearlly observe the data imbalance in this data so now lets do the undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aadb638",
   "metadata": {},
   "source": [
    "### Undersampling the data to get the balace in the data\n",
    "\n",
    "The reason for undersampling is that there are more observations in the data. So, undersampling th data can acheive the data balance and also helps us remove extra data\n",
    "\n",
    "Lets use the random under sampler to undersample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "145dd7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "undersample = RandomUnderSampler(sampling_strategy='majority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b363e8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train=undersample.fit_resample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1795c59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fire_Alarm\n",
       "0             12420\n",
       "1             12420\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538b4500",
   "metadata": {},
   "source": [
    "Now the data is balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df632f71",
   "metadata": {},
   "source": [
    "### Deciding on the best evalution metrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bf2c55",
   "metadata": {},
   "source": [
    "Our main aim is to reduce the False Alarms which are nothing but the False Positives(Detected the smoke but there is actualy no smoke). But neglectng the False Negatives is even more dangerous as it doesn't detect smoke but there is actually smoke which could potentially be a fire.\n",
    "\n",
    "Which means we have deal with both False Negatives and False Positives and the best evalution metric for this is **'F1 SCORE'**.\n",
    "F1 score is the harmonic mean of Recall and precision so it deals with both false negatives and false postives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b7a7a0",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0681dc93",
   "metadata": {},
   "source": [
    "Lets Create a data frame to store the results of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dc20b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame({\"model\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb59bed",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "\n",
    "#### Without Randomsearch and gridsearch cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "243389d5",
   "metadata": {
    "id": "5WfGTWb3hYd-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 43.9 s\n",
      "Wall time: 22.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ann = MLPClassifier(hidden_layer_sizes=(60,50,40), solver='adam', max_iter=200)\n",
    "_ = ann.fit(X_train,np.ravel( y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a421f866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 22.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = ann.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "754ac0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5453\n",
      "           1       1.00      1.00      1.00     13336\n",
      "\n",
      "    accuracy                           1.00     18789\n",
      "   macro avg       1.00      1.00      1.00     18789\n",
      "weighted avg       1.00      1.00      1.00     18789\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1695e92e",
   "metadata": {},
   "source": [
    "## With RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb384edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1096: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'sgd', 'max_iter': 3000, 'learning_rate_init': 0.2, 'learning_rate': 'adaptive', 'hidden_layer_sizes': (40, 20), 'alpha': 0, 'activation': 'tanh'}\n",
      "CPU times: total: 1min 10s\n",
      "Wall time: 19min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "score_measure = \"f1\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [ (50,), (70,),(50,30), (40,20), (60,40, 20), (70,50,40)],\n",
    "    'activation': ['logistic', 'tanh', 'relu'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0, .2, .5, .7, 1],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'learning_rate_init': [0.001, 0.01, 0.1, 0.2, 0.5],\n",
    "    'max_iter': [3000]\n",
    "}\n",
    "\n",
    "ann = MLPClassifier()\n",
    "grid_search = RandomizedSearchCV(estimator = ann, param_distributions=param_grid, cv=kfolds, n_iter=100,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_\n",
    "\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b0a558f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5453\n",
      "           1       1.00      1.00      1.00     13336\n",
      "\n",
      "    accuracy                           1.00     18789\n",
      "   macro avg       1.00      1.00      1.00     18789\n",
      "weighted avg       1.00      1.00      1.00     18789\n",
      "\n",
      "CPU times: total: 93.8 ms\n",
      "Wall time: 82.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = bestRecallTree.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819af11d",
   "metadata": {},
   "source": [
    "## With GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d015048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1096: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'alpha': 0, 'hidden_layer_sizes': (50, 90), 'learning_rate': 'adaptive', 'learning_rate_init': 0.2, 'max_iter': 5000, 'solver': 'sgd'}\n",
      "CPU times: total: 1min 16s\n",
      "Wall time: 22min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "score_measure = \"f1\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [ (50,30), (50,70), (50,90)],\n",
    "    'activation': ['logistic', 'relu'],\n",
    "    'solver': ['sgd'],\n",
    "    'alpha': [0,.5 ],\n",
    "    'learning_rate': ['adaptive', 'invscaling'],\n",
    "    'learning_rate_init': [0.1,0.2,0.25],\n",
    "    'max_iter': [5000]\n",
    "}\n",
    "\n",
    "ann = MLPClassifier()\n",
    "grid_search = GridSearchCV(estimator = ann, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_\n",
    "\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54a95689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5453\n",
      "           1       1.00      1.00      1.00     13336\n",
      "\n",
      "    accuracy                           1.00     18789\n",
      "   macro avg       1.00      1.00      1.00     18789\n",
      "weighted avg       1.00      1.00      1.00     18789\n",
      "\n",
      "CPU times: total: 141 ms\n",
      "Wall time: 102 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = bestRecallTree.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1dd4320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.9986162 Precision=0.9989504 Recall=0.9991002 F1=0.9990253\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, y_pred)\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")\n",
    "F1_lr=2*TP/(2*TP+FP+FN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f2887cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score from the Neural Networks model using Random Search and Grid Search is :0.9990252680512859\n"
     ]
    }
   ],
   "source": [
    "print(f\"The F1 score from the Neural Networks model using Random Search and Grid Search is :{F1_lr}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecb22c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.concat([performance, pd.DataFrame({'model':\"neural network using random & grid search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83622125",
   "metadata": {},
   "source": [
    "### Logistic Regression Model\n",
    "\n",
    "#### Logistic Regression model using Random Search and Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cbe109b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "840 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "270 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "250 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\joblib\\parallel.py\", line 1048, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\joblib\\parallel.py\", line 864, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\joblib\\parallel.py\", line 782, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "320 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 71, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.85252436 0.90669682        nan 0.9060427  0.90613967        nan\n",
      " 0.90604301 0.89017644 0.90654873 0.90656416 0.84533901 0.89207672\n",
      " 0.85252436 0.90652836        nan 0.89159831        nan 0.8995133\n",
      " 0.90656546        nan 0.9063739  0.9063798  0.90668957 0.90656416\n",
      "        nan        nan        nan        nan 0.89263545        nan\n",
      "        nan 0.90653591 0.90652719 0.90664502 0.89263545        nan\n",
      " 0.90649899 0.89207672        nan        nan 0.89159831 0.90613967\n",
      "        nan 0.90638215        nan        nan 0.89263545 0.90613967\n",
      " 0.85252436 0.8995133  0.90613967        nan 0.90641578        nan\n",
      " 0.90649948 0.90613967 0.85252436 0.90638533        nan 0.89017644\n",
      " 0.8995133         nan 0.9060173  0.81757657 0.9063798         nan\n",
      " 0.89263545 0.90662361 0.90616634 0.89017644 0.89207672 0.90657154\n",
      " 0.9063798         nan 0.8995133  0.90649203        nan        nan\n",
      " 0.89207672 0.81754247 0.90656319 0.90598021        nan        nan\n",
      " 0.90612937 0.89159831 0.90633771 0.81757657        nan 0.8995133\n",
      "        nan 0.90602434 0.90648491 0.90634507        nan 0.89263545\n",
      " 0.90653543 0.9063798  0.89263545 0.89951378 0.81754247        nan\n",
      " 0.90642249 0.9060427         nan 0.84533901        nan        nan\n",
      " 0.90657189        nan 0.9064553         nan        nan 0.8995133\n",
      " 0.9063798  0.89207672 0.9060173  0.85252436 0.89951378 0.89207672\n",
      "        nan        nan 0.90664502        nan 0.9063798         nan\n",
      "        nan 0.90661614 0.90652719        nan        nan 0.90626965\n",
      " 0.90629266        nan 0.89263545 0.89017644        nan        nan\n",
      "        nan 0.89159831 0.89159831        nan        nan        nan\n",
      "        nan        nan        nan 0.9060427         nan 0.89207672\n",
      "        nan 0.81236847 0.90642249 0.90657255 0.90642308 0.8452965\n",
      "        nan 0.90657189        nan 0.90660025 0.89017644        nan\n",
      "        nan 0.90634507        nan        nan 0.81236847 0.8995133\n",
      " 0.90666033 0.90613967 0.9060173  0.90660903 0.90652836 0.9063798\n",
      "        nan 0.906006   0.90598071 0.89207672 0.90657189        nan\n",
      "        nan        nan        nan 0.90602434        nan 0.90598021\n",
      "        nan 0.8452965         nan        nan 0.81233854 0.89017644\n",
      "        nan        nan        nan        nan 0.81754247 0.84533901\n",
      " 0.90634262 0.90666033 0.81754247 0.90653591 0.90653591 0.89017644\n",
      "        nan 0.90648491        nan 0.90630019 0.90648933        nan\n",
      " 0.90598021 0.89017644        nan 0.90669682        nan 0.8995133\n",
      " 0.90613967        nan        nan 0.89951378 0.90613967 0.89951378\n",
      " 0.89951378 0.90643802 0.89951378        nan 0.81236847 0.81236847\n",
      "        nan 0.90630756 0.9064553  0.90660823 0.906006   0.9063798\n",
      " 0.90649899 0.90653591        nan 0.8452965         nan        nan\n",
      "        nan 0.90665343 0.90644796 0.90661614 0.81236847 0.89207672\n",
      " 0.90652878        nan 0.90653591        nan 0.8995133  0.90657189\n",
      " 0.89159831 0.90638874 0.89263545 0.81236847 0.90613967 0.89207672\n",
      " 0.90634896 0.90649899 0.85252436        nan        nan 0.9060427\n",
      " 0.8452965  0.90642249        nan        nan 0.81757657        nan\n",
      " 0.81236847 0.90613967 0.90594361 0.8995133  0.90609241 0.81757657\n",
      "        nan        nan 0.90645229 0.89263545        nan 0.90652599\n",
      " 0.90665286 0.8452965  0.8995133  0.90638533 0.90652719 0.90641564\n",
      " 0.90644566 0.90613967 0.81754247 0.8995133  0.90613967 0.90648933\n",
      " 0.81236847 0.90642565 0.90613758 0.89951378 0.89017644 0.90653543\n",
      " 0.89951378        nan 0.8995133         nan 0.906006          nan\n",
      " 0.89159831 0.90627003 0.90638874        nan 0.9064553  0.89951378\n",
      "        nan 0.9063798         nan        nan        nan        nan\n",
      "        nan 0.9063798  0.9060427  0.81236847 0.90643802        nan\n",
      " 0.90653591 0.90638874 0.89159831 0.9060173  0.90661695 0.90657189\n",
      " 0.89951378 0.8995133         nan 0.89263545        nan 0.89207672\n",
      " 0.89017644 0.89159831        nan 0.90661695 0.89263545 0.90654873\n",
      " 0.90634262        nan        nan        nan        nan 0.90669682\n",
      "        nan 0.90629268        nan 0.90610085 0.90669682 0.9060173\n",
      " 0.90613967 0.89263545        nan        nan 0.90666112 0.90665343\n",
      " 0.90656319        nan 0.89207672 0.906006   0.90654873 0.89207672\n",
      " 0.90662361 0.90627003 0.9063798  0.90645229 0.89159831 0.90669782\n",
      "        nan 0.90657088        nan 0.81236847 0.90649203        nan\n",
      "        nan 0.89207672 0.9064553         nan 0.90609211        nan\n",
      "        nan 0.90644796 0.89017644        nan 0.906006   0.90613967\n",
      "        nan        nan 0.90630105        nan 0.90653487 0.90644592\n",
      "        nan 0.9063798  0.89263545        nan 0.8452965  0.90642249\n",
      " 0.90653543 0.84533901 0.90598071 0.90613967        nan 0.90644823\n",
      " 0.90654314 0.90653543 0.90656546        nan 0.8995133  0.90666112\n",
      " 0.90654873 0.90638612 0.9060427         nan 0.90660025 0.90657189\n",
      " 0.89951378 0.8995133  0.90619693        nan        nan 0.81754247\n",
      "        nan        nan 0.90644566 0.90642249 0.85252436 0.90656416\n",
      "        nan 0.9063798         nan        nan 0.90598071        nan\n",
      " 0.89951378 0.90643064 0.89017644        nan 0.90613967        nan\n",
      "        nan 0.9060173         nan        nan 0.9063798  0.9060173\n",
      "        nan 0.81757657        nan 0.90641564        nan        nan\n",
      " 0.81236847        nan 0.9060427  0.89159831        nan        nan\n",
      "        nan 0.8995133         nan        nan 0.9063798  0.90656546\n",
      " 0.8995133  0.89263545        nan 0.81236847        nan 0.90593783\n",
      "        nan        nan 0.90661572 0.81757657 0.81236847 0.8452965\n",
      " 0.906006   0.89159831 0.90644796 0.89951378 0.90643802 0.8452965\n",
      " 0.89017644 0.89951378 0.89207672 0.9063798  0.90657189 0.90605648\n",
      " 0.81236847 0.90660025        nan 0.90657189        nan 0.9062562\n",
      "        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.9066978169406108\n",
      "... with parameters: {'solver': 'saga', 'penalty': 'none', 'max_iter': 711, 'C': 0.1}\n",
      "CPU times: total: 11.7 s\n",
      "Wall time: 14min 43s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "score_measure = \"f1\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {'C':[0.001,0.01,0.1,1,10], # C is the regulization strength\n",
    "               'penalty':['l1', 'l2','elasticnet','none'],\n",
    "              'solver':['saga','liblinear'],\n",
    "              'max_iter': np.arange(500,1000)\n",
    "                  \n",
    "}\n",
    "\n",
    "lg = LogisticRegression()\n",
    "rand_search = RandomizedSearchCV(estimator =lg, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1  # n_jobs=-1 will utilize all available CPUs \n",
    "                                )\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestlogestic = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f7319bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 600 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.9066978169406108\n",
      "... with parameters: {'C': 0.05, 'max_iter': 709, 'penalty': 'none', 'solver': 'saga'}\n",
      "CPU times: total: 14.8 s\n",
      "Wall time: 57min 2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "score_measure = \"f1\"\n",
    "kfolds = 5\n",
    "best_penality = rand_search.best_params_['penalty']\n",
    "best_solver = rand_search.best_params_['solver']\n",
    "min_regulization_strength=rand_search.best_params_['C']\n",
    "min_iter = rand_search.best_params_['max_iter']\n",
    "\n",
    "#Using the best parameters from the Random Search to use as range for the parameters to do the grid search\n",
    "param_grid = {\n",
    "    \n",
    "    'C':np.arange(min_regulization_strength-0.05,min_regulization_strength+0.05), \n",
    "               'penalty':[best_penality],\n",
    "              'solver':[best_solver],\n",
    "              'max_iter': np.arange(min_iter-300,min_iter+300)\n",
    "}\n",
    "\n",
    "lgr =  LogisticRegression()\n",
    "grid_search = GridSearchCV(estimator = lgr, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1 # n_jobs=-1 will utilize all available CPUs \n",
    "                )\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestlgr = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85382c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.9054234 Precision=0.9641796 Recall=0.9001950 F1=0.9310893\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")\n",
    "F1_lr=2*TP/(2*TP+FP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f844e46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score from the Logestic Regression model using Random Search and Grid Search is :0.9310893085663319\n"
     ]
    }
   ],
   "source": [
    "print(f\"The F1 score from the Logestic Regression model using Random Search and Grid Search is :{F1_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db571ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.concat([performance, pd.DataFrame({'model':\"logistic using random & grid search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e519f31",
   "metadata": {},
   "source": [
    "### SVM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed467f1",
   "metadata": {},
   "source": [
    "#### SVM using RandomSearch and Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "917d045a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 500 candidates, totalling 1500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.9995169859781171\n",
      "... with parameters: {'kernel': 'poly', 'gamma': 'scale', 'degree': 4, 'coef0': 2, 'C': 50.1}\n",
      "CPU times: total: 17.7 s\n",
      "Wall time: 5h 5min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "score_measure = \"f1\"\n",
    "kfolds = 3\n",
    "\n",
    "param_grid = {'C':np.arange(0.1,100,10),  #  regularization parameter.\n",
    "               'kernel':['linear', 'rbf','poly'],\n",
    "              'gamma':['scale','auto'],\n",
    "              'degree':np.arange(1,10), #degree is for the polynomial kernal\n",
    "              'coef0':np.arange(1,10) #coef0 is for the polynomial kernal\n",
    "                  \n",
    "}\n",
    "\n",
    "svc = SVC()\n",
    "rand_search = RandomizedSearchCV(estimator =svc, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1  # n_jobs=-1 will utilize all available CPUs \n",
    "                                )\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestsvc = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61dc06bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arihe\\anaconda3_new\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.9995572387656226\n",
      "... with parameters: {'C': 47.1, 'coef0': 2, 'degree': 4, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "CPU times: total: 2.72 s\n",
      "Wall time: 4min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "score_measure = \"f1\"\n",
    "kfolds = 3\n",
    "best_kernel = rand_search.best_params_['kernel']\n",
    "best_gamma = rand_search.best_params_['gamma']\n",
    "min_regulization=rand_search.best_params_['C']\n",
    "best_degree = rand_search.best_params_['degree']\n",
    "best_coef0=rand_search.best_params_['coef0']\n",
    "\n",
    "#Using the best parameters from the Random Search to use as range for the parameters to do the grid search\n",
    "param_grid = {\n",
    "    \n",
    "    'C':np.arange(min_regulization-3,min_regulization+3), \n",
    "               'kernel':[best_kernel],\n",
    "              'gamma':[best_gamma],\n",
    "              'degree': np.arange(best_degree-1,best_degree+1),\n",
    "            'coef0': np.arange(best_coef0-3,best_coef0+3)\n",
    "}\n",
    "\n",
    "svm_grid =  SVC()\n",
    "grid_search = GridSearchCV(estimator = svm_grid, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1 # n_jobs=-1 will utilize all available CPUs \n",
    "                )\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "best_svm = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99658055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.9996274 Precision=0.9997750 Recall=0.9997001 F1=0.9997375\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")\n",
    "F1_svm=2*TP/(2*TP+FP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85adf5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 score from the SVM model using Random Search and Grid Search is 0.9997375426493195\n"
     ]
    }
   ],
   "source": [
    "print(f\"The f1 score from the SVM model using Random Search and Grid Search is {F1_svm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92570f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.concat([performance, pd.DataFrame({'model':\"svm using Random & Grid search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2104cfaa",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e53ffb",
   "metadata": {},
   "source": [
    "#### Decision Trees using RandomSearchCV combined with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9110d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best f1 score is 0.9993154821171236\n",
      "... with parameters: {'min_samples_split': 3, 'min_samples_leaf': 4, 'min_impurity_decrease': 0.0001, 'max_leaf_nodes': 21, 'max_depth': 13, 'criterion': 'entropy'}\n",
      "CPU times: total: 9.45 s\n",
      "Wall time: 59.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "score_measure = \"f1\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,100),  \n",
    "    'min_samples_leaf': np.arange(1,100),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 100), \n",
    "    'max_depth': np.arange(1,25), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29c8ed75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1024 candidates, totalling 5120 fits\n",
      "The best f1 score is 0.9995973100074999\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 11, 'max_leaf_nodes': 22, 'min_impurity_decrease': 5e-05, 'min_samples_leaf': 2, 'min_samples_split': 3}\n",
      "CPU times: total: 19.2 s\n",
      "Wall time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "score_measure = \"f1\"\n",
    "kfolds = 5\n",
    "min_samples_split = rand_search.best_params_['min_samples_split']\n",
    "min_samples_leaf = rand_search.best_params_['min_samples_leaf']\n",
    "min_impurity_decrease = rand_search.best_params_['min_impurity_decrease']\n",
    "max_leaf_nodes = rand_search.best_params_['max_leaf_nodes']\n",
    "max_depth = rand_search.best_params_['max_depth']\n",
    "criterion = rand_search.best_params_['criterion']\n",
    "#Using the best parameters from the Random Search to use as range for the parameters to do the grid search\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(min_samples_split-2,min_samples_split+2),  \n",
    "    'min_samples_leaf': np.arange(min_samples_leaf-2,min_samples_leaf+2),\n",
    "    'min_impurity_decrease': np.arange(min_impurity_decrease-0.0001, min_impurity_decrease+0.0001, 0.00005),\n",
    "    'max_leaf_nodes': np.arange(max_leaf_nodes-2,max_leaf_nodes+2), \n",
    "    'max_depth': np.arange(max_depth-2,max_depth+2), \n",
    "    'criterion': [criterion]\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = dtree, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d78e1c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.9994146 Precision=0.9998500 Recall=0.9993251 F1=0.9995875\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")\n",
    "F1_Decisiontree=2*TP/(2*TP+FP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27905dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 score from the Decision Tree using Random Search and Grid Search is 0.9995874742171386\n"
     ]
    }
   ],
   "source": [
    "print(f\"The f1 score from the Decision Tree using Random Search and Grid Search is {F1_Decisiontree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "042d0315",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Decision Tree\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac50f49",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d82fc99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neural network using random &amp; grid search</td>\n",
       "      <td>0.998616</td>\n",
       "      <td>0.998950</td>\n",
       "      <td>0.999100</td>\n",
       "      <td>0.999025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic using random &amp; grid search</td>\n",
       "      <td>0.905423</td>\n",
       "      <td>0.964180</td>\n",
       "      <td>0.900195</td>\n",
       "      <td>0.931089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm using Random &amp; Grid search</td>\n",
       "      <td>0.999627</td>\n",
       "      <td>0.999775</td>\n",
       "      <td>0.999700</td>\n",
       "      <td>0.999738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.999415</td>\n",
       "      <td>0.999850</td>\n",
       "      <td>0.999325</td>\n",
       "      <td>0.999587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       model  Accuracy  Precision    Recall  \\\n",
       "0  neural network using random & grid search  0.998616   0.998950  0.999100   \n",
       "0        logistic using random & grid search  0.905423   0.964180  0.900195   \n",
       "0             svm using Random & Grid search  0.999627   0.999775  0.999700   \n",
       "0                              Decision Tree  0.999415   0.999850  0.999325   \n",
       "\n",
       "         F1  \n",
       "0  0.999025  \n",
       "0  0.931089  \n",
       "0  0.999738  \n",
       "0  0.999587  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5120c6f1",
   "metadata": {},
   "source": [
    "We can observe that th F1 score of the Support Vector Machine model is more than the other models we used to performe the analysis. But there is very slight difference between the Neural Network and SVM and Decision Tree from which we can say that all these three models are a good fit for this data. \n",
    "\n",
    "We can say that the neural network is alos fitting the data very well and having much almost equal values to the SVM. But as Neural networks are particularly good at learning complex patterns and relationships in large datasets and our data set is about detecting fire which will have many complex relationships between the attributes and potentially more noise model  . As svm model is not good in handeling noise and decision tree model is not good in complex relations we can conclude that the **Neural Network** as the best model for our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f157f8a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
