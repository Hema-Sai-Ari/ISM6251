{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c2508d",
   "metadata": {},
   "source": [
    "## Model Fitting\n",
    "\n",
    "Lets fit the Logestic Regression Model, Support vector Machine, Decision Tree, Neural Networks and the Deep Neural Networks(DNN) using keras models on this data-set and do the analysis and get the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f79772",
   "metadata": {},
   "source": [
    "### Importing the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9026ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "\n",
    "\n",
    "# setting random seed to ensure that results are repeatable\n",
    "np.random.seed(7026)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac44028",
   "metadata": {},
   "source": [
    "### Importing the train and test data set from the Data processing notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01cda7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.read_csv(\"smoke_train_X.csv\")\n",
    "X_test=pd.read_csv(\"smoke_test_X.csv\")\n",
    "y_train=pd.read_csv(\"smoke_train_y.csv\")\n",
    "y_test=pd.read_csv(\"smoke_test_y.csv\")\n",
    "\n",
    "train_df=pd.read_csv(\"smoke_train_df.csv\")\n",
    "test_df=pd.read_csv(\"smoke_test_y.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d71d1",
   "metadata": {},
   "source": [
    "### Standardizing the variables\n",
    "\n",
    "We standardize our variables to eliminate the differences in scale between the variables/attributes.\n",
    "\n",
    "We will use the sklearn library's 'standard scaler' to accomplish this. The standard scaler function will standardize our variables. To achieve this, we will first need to train the scaler on the training data and then apply this trained scaler to standardize both the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49e8dc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform the predictors of training and test sets\n",
    "X_train = scaler.transform(X_train) \n",
    " \n",
    "\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1474d59",
   "metadata": {},
   "source": [
    "### Checking for the Imbalance in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8263ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    31421\n",
       "0    12420\n",
       "Name: Fire_Alarm, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Fire_Alarm.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422e9513",
   "metadata": {},
   "source": [
    "We can clearlly observe the data imbalance in this data so now lets do the undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aadb638",
   "metadata": {},
   "source": [
    "### Undersampling the data to get the balace in the data\n",
    "\n",
    "The reason for undersampling is that there are more observations in the data. So, undersampling th data can acheive the data balance and also helps us remove extra data\n",
    "\n",
    "Lets use the random under sampler to undersample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "145dd7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "undersample = RandomUnderSampler(sampling_strategy='majority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b363e8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train=undersample.fit_resample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1795c59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fire_Alarm\n",
       "0             12420\n",
       "1             12420\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538b4500",
   "metadata": {},
   "source": [
    "Now the data is balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df632f71",
   "metadata": {},
   "source": [
    "### Deciding on the best evalution metrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bf2c55",
   "metadata": {},
   "source": [
    "Our main aim is to reduce the False Alarms which are nothing but the False Positives(Detected the smoke but there is actualy no smoke). But neglectng the False Negatives is even more dangerous as it doesn't detect smoke but there is actually smoke which could potentially be a fire.\n",
    "\n",
    "Which means we have deal with both False Negatives and False Positives and the best evalution metric for this is **'F1 SCORE'**.\n",
    "F1 score is the harmonic mean of Recall and precision so it deals with both false negatives and false postives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b7a7a0",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0681dc93",
   "metadata": {},
   "source": [
    "Lets Create a data frame to store the results of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dc20b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame({\"model\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83622125",
   "metadata": {},
   "source": [
    "### Logistic Regression Model\n",
    "\n",
    "#### Logistic Regression model using Random Search and Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cbe109b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "1020 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "355 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "335 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 71, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "330 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.90665343 0.90669782 0.90613967 0.89017644 0.90666112\n",
      " 0.90654873        nan        nan 0.8995133  0.8452965         nan\n",
      " 0.9064924  0.90653543 0.89017644        nan        nan 0.85252436\n",
      "        nan        nan 0.90641564 0.90665343        nan 0.89017644\n",
      "        nan 0.84532772        nan        nan 0.89951378        nan\n",
      " 0.90631297 0.90652719        nan 0.8995133  0.90613967 0.90665343\n",
      "        nan 0.90623339        nan 0.90666033 0.8452965  0.81236847\n",
      "        nan 0.90593634 0.90644823        nan        nan        nan\n",
      " 0.8995133  0.89951378 0.90654338 0.90660876 0.89017644 0.90616634\n",
      "        nan 0.906006   0.90613967        nan 0.9062644  0.90660027\n",
      "        nan 0.906006          nan 0.90640157        nan        nan\n",
      " 0.85252436 0.90660027        nan        nan 0.89951378        nan\n",
      " 0.89951378 0.90630613 0.90602434 0.90593584        nan 0.9063798\n",
      "        nan 0.90642249 0.90652846        nan 0.85252436        nan\n",
      "        nan 0.90642249 0.89263545        nan 0.90669682 0.906006\n",
      "        nan        nan        nan 0.9064553         nan 0.90660794\n",
      "        nan 0.90597475 0.906006   0.8452965         nan 0.89207672\n",
      "        nan 0.90630613 0.90666033 0.81754247 0.89159831 0.81236847\n",
      "        nan 0.90649899 0.81754247 0.89159831 0.89017644 0.90648896\n",
      "        nan        nan        nan        nan        nan 0.85252436\n",
      " 0.90657236        nan        nan        nan        nan        nan\n",
      " 0.89951378 0.90638591 0.90645966 0.89951378 0.90629983        nan\n",
      "        nan        nan 0.89263545        nan 0.9063798         nan\n",
      " 0.90648491 0.9064553         nan        nan        nan        nan\n",
      " 0.90666112        nan 0.8995133  0.9063798  0.90657943 0.89017644\n",
      "        nan 0.89263545 0.90605648 0.90656546 0.9064553         nan\n",
      "        nan        nan        nan        nan 0.90652719 0.90609253\n",
      " 0.90612293 0.8995133         nan 0.81236847        nan        nan\n",
      " 0.90649898        nan 0.81754247        nan 0.90654873        nan\n",
      " 0.90661614        nan 0.90657154 0.90630814 0.89951378 0.90623339\n",
      " 0.89263545 0.90666112 0.81754247        nan 0.90657236        nan\n",
      " 0.90666033 0.89951378        nan 0.8995133         nan        nan\n",
      " 0.85252436        nan 0.9064188         nan 0.8452965  0.85252436\n",
      " 0.8452965  0.84533901        nan        nan        nan        nan\n",
      "        nan        nan 0.90669782        nan        nan 0.89263545\n",
      "        nan        nan        nan 0.9060173         nan        nan\n",
      "        nan 0.90653543        nan 0.89017644        nan        nan\n",
      "        nan        nan 0.84533901 0.89159831        nan 0.89951378\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.89951378 0.81754247 0.90665343 0.81236847 0.89159831        nan\n",
      " 0.81236847        nan        nan 0.90627003 0.89951378 0.85252436\n",
      "        nan        nan 0.90612937 0.90644823 0.90642565 0.89951378\n",
      " 0.90653543 0.90642249 0.90652719        nan 0.90619655 0.90644796\n",
      " 0.89951378 0.90661637 0.89159831        nan 0.81236847 0.9060173\n",
      " 0.90643802        nan 0.81236847 0.81236847 0.90669782 0.90661695\n",
      "        nan 0.90630105 0.81236847        nan 0.90666033        nan\n",
      "        nan 0.90586442        nan 0.90630814 0.90651172 0.85252436\n",
      " 0.85252436        nan 0.90665343        nan 0.90649948 0.90645966\n",
      " 0.81236847        nan        nan        nan 0.89951378 0.90660027\n",
      " 0.90643066 0.90634458 0.90657189        nan 0.90649203 0.90613967\n",
      " 0.9062644         nan        nan 0.89207672 0.90666033 0.81236847\n",
      " 0.89263545 0.90646205 0.89017644 0.9063798         nan        nan\n",
      " 0.9060173  0.90619693        nan 0.89263545 0.90613967        nan\n",
      "        nan 0.90604301        nan 0.89263545        nan 0.9060173\n",
      " 0.90597997 0.90644823 0.90622644 0.90613967 0.90659971 0.90649898\n",
      "        nan        nan        nan        nan        nan 0.81236847\n",
      " 0.90654314 0.90644823 0.90597447 0.90649898        nan 0.89017644\n",
      " 0.81754247 0.90658025 0.8995133         nan        nan        nan\n",
      " 0.89263545 0.89159831        nan 0.81236847 0.906006   0.90613967\n",
      "        nan 0.90629268 0.90644823        nan        nan 0.90653621\n",
      " 0.90651172 0.9060173  0.81757657        nan 0.90613006 0.85252436\n",
      " 0.81236847        nan 0.90637912 0.90660027        nan 0.90616634\n",
      "        nan 0.90660834        nan 0.89263545 0.89951378        nan\n",
      " 0.90616567        nan 0.81236847 0.90653591        nan 0.90654273\n",
      " 0.9060427  0.90664502        nan        nan        nan 0.89207672\n",
      " 0.89207672 0.89207672 0.89951378        nan 0.85252436        nan\n",
      " 0.8995133         nan 0.81757657 0.90652719        nan        nan\n",
      "        nan        nan 0.89017644        nan        nan 0.90644823\n",
      " 0.90605559 0.90661572 0.90649898        nan 0.89017644        nan\n",
      " 0.90657189 0.90660027 0.90653543 0.9060427  0.9062562  0.90653543\n",
      "        nan 0.90630702        nan        nan 0.90646766        nan\n",
      "        nan        nan        nan        nan 0.90653543 0.9063798\n",
      " 0.89017644 0.90650668 0.89017644 0.8995133  0.90605552 0.90627852\n",
      "        nan 0.90664493        nan 0.90656359 0.9060173         nan\n",
      " 0.89017644        nan 0.90634507 0.90642249        nan 0.8995133\n",
      "        nan 0.90656416 0.90666071 0.90638533 0.90657154        nan\n",
      "        nan        nan 0.90662361 0.89159831        nan 0.89263545\n",
      " 0.8452965         nan        nan        nan 0.90657189 0.9063798\n",
      "        nan 0.90619693        nan        nan        nan        nan\n",
      " 0.90656416 0.90641578 0.906006   0.81757657 0.90669782 0.9063798\n",
      " 0.90613967 0.89017644 0.90630702 0.90646727 0.81236847        nan\n",
      "        nan 0.906006          nan 0.90669682        nan 0.90660066\n",
      "        nan 0.90598071        nan 0.9060173  0.90653591 0.906006\n",
      " 0.9060173  0.90613967]\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.9066978169406108\n",
      "... with parameters: {'solver': 'saga', 'penalty': 'l2', 'max_iter': 785, 'C': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {'C':[0.001,0.01,0.1,1,10], # C is the regulization strength\n",
    "               'penalty':['l1', 'l2','elasticnet','none'],\n",
    "              'solver':['saga','liblinear'],\n",
    "              'max_iter': np.arange(500,1000)\n",
    "                  \n",
    "}\n",
    "\n",
    "lg = LogisticRegression()\n",
    "rand_search = RandomizedSearchCV(estimator =lg, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1  # n_jobs=-1 will utilize all available CPUs \n",
    "                                )\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestlogestic = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f7319bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 600 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.90672653110977\n",
      "... with parameters: {'C': 9.95, 'max_iter': 721, 'penalty': 'l2', 'solver': 'saga'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 5\n",
    "best_penality = rand_search.best_params_['penalty']\n",
    "best_solver = rand_search.best_params_['solver']\n",
    "min_regulization_strength=rand_search.best_params_['C']\n",
    "min_iter = rand_search.best_params_['max_iter']\n",
    "\n",
    "#Using the best parameters from the Random Search to use as range for the parameters to do the grid search\n",
    "param_grid = {\n",
    "    \n",
    "    'C':np.arange(min_regulization_strength-0.05,min_regulization_strength+0.05), \n",
    "               'penalty':[best_penality],\n",
    "              'solver':[best_solver],\n",
    "              'max_iter': np.arange(min_iter-300,min_iter+300)\n",
    "}\n",
    "\n",
    "lgr =  LogisticRegression()\n",
    "grid_search = GridSearchCV(estimator = lgr, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1 # n_jobs=-1 will utilize all available CPUs \n",
    "                )\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestlgr = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85382c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.9052637 Precision=0.9638728 Recall=0.9002699 F1=0.9309864\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")\n",
    "F1_lr=2*TP/(2*TP+FP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f844e46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score from the Logestic Regression model using Random Search and Grid Search is :0.9309863523573201\n"
     ]
    }
   ],
   "source": [
    "print(f\"The F1 score from the Logestic Regression model using Random Search and Grid Search is :{F1_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db571ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.concat([performance, pd.DataFrame({'model':\"logistic using random & grid search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e519f31",
   "metadata": {},
   "source": [
    "### SVM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed467f1",
   "metadata": {},
   "source": [
    "#### SVM using RandomSearch and Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "917d045a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 500 candidates, totalling 1500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.9995170442896705\n",
      "... with parameters: {'kernel': 'poly', 'gamma': 'auto', 'degree': 3, 'coef0': 8, 'C': 90.1}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 3\n",
    "\n",
    "param_grid = {'C':np.arange(0.1,100,10),  #  regularization parameter.\n",
    "               'kernel':['linear', 'rbf','poly'],\n",
    "              'gamma':['scale','auto'],\n",
    "              'degree':np.arange(1,10), #degree is for the polynomial kernal\n",
    "              'coef0':np.arange(1,10) #coef0 is for the polynomial kernal\n",
    "                  \n",
    "}\n",
    "\n",
    "svc = SVC()\n",
    "rand_search = RandomizedSearchCV(estimator =svc, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1  # n_jobs=-1 will utilize all available CPUs \n",
    "                                )\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestsvc = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61dc06bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.9995170442896705\n",
      "... with parameters: {'C': 87.1, 'coef0': 8, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 3\n",
    "best_kernel = rand_search.best_params_['kernel']\n",
    "best_gamma = rand_search.best_params_['gamma']\n",
    "min_regulization=rand_search.best_params_['C']\n",
    "best_degree = rand_search.best_params_['degree']\n",
    "best_coef0=rand_search.best_params_['coef0']\n",
    "\n",
    "#Using the best parameters from the Random Search to use as range for the parameters to do the grid search\n",
    "param_grid = {\n",
    "    \n",
    "    'C':np.arange(min_regulization-3,min_regulization+3), \n",
    "               'kernel':[best_kernel],\n",
    "              'gamma':[best_gamma],\n",
    "              'degree': np.arange(best_degree-1,best_degree+1),\n",
    "            'coef0': np.arange(best_coef0-3,best_coef0+3)\n",
    "}\n",
    "\n",
    "svm_grid =  SVC()\n",
    "grid_search = GridSearchCV(estimator = svm_grid, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1 # n_jobs=-1 will utilize all available CPUs \n",
    "                )\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "best_svm = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99658055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.9995210 Precision=0.9997000 Recall=0.9996251 F1=0.9996626\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")\n",
    "F1_svm=2*TP/(2*TP+FP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85adf5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 score from the SVM model using Random Search and Grid Search is 0.9996625548348393\n"
     ]
    }
   ],
   "source": [
    "print(f\"The f1 score from the SVM model using Random Search and Grid Search is {F1_svm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92570f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.concat([performance, pd.DataFrame({'model':\"svm using Random & Grid search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2104cfaa",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e53ffb",
   "metadata": {},
   "source": [
    "#### Decision Trees using RandomSearchCV combined with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9110d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "30 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 889, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 177, in fit\n",
      "    self._validate_params()\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py\", line 600, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_split' parameter of DecisionTreeClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.99823158 0.83714789 0.99823158 0.99794818 0.98600228 0.99823158\n",
      " 0.98622193 0.99823158 0.99822876 0.99794818 0.99823158 0.99790158\n",
      " 0.92801481 0.99823158 0.99750465 0.99895369 0.99823158 0.99669757\n",
      " 0.92801481 0.99859081 0.93220934 0.99851008 0.99895369 0.99826891\n",
      " 0.99722183 0.9966972  0.98908872 0.99802837 0.99822963 0.99786743\n",
      " 0.9976658  0.99823158 0.99823158 0.99810908 0.99895369 0.99130371\n",
      " 0.99826891 0.99474116 0.98908872 0.99553908 0.99823158 0.99738373\n",
      " 0.98598172 0.99566054 0.9941858  0.98908872 0.99846964 0.99242839\n",
      " 0.99907437 0.99823158 0.99782717 0.99931561 0.83714789 0.99486822\n",
      " 0.99827155 0.99810979 0.99730287 0.99863035 0.99827181 0.92801481\n",
      " 0.93220934 0.99814931 0.83714789 0.98600228 0.99823158 0.99823158\n",
      " 0.93220934 0.99827181 0.98600228 0.99561895 0.99823158 0.83714789\n",
      " 0.99818827 0.9947812  0.99851008 0.9975852  0.99827181 0.99823158\n",
      " 0.99522884 0.99823158 0.99879222 0.99823158 0.99823158 0.93220934\n",
      " 0.93220934 0.98908872 0.99506463 0.99834743 0.99919457 0.99810908\n",
      " 0.99838841 0.99895369 0.99823158 0.99806922 0.99859081 0.99823158\n",
      " 0.93220934 0.99823158 0.99482105 0.99325449 0.98895599 0.99814931\n",
      " 0.99823158 0.99855024 0.99823158 0.99919452 0.99317692 0.99702021\n",
      " 0.99823158 0.83714789 0.99823158 0.9949411  0.99883166 0.83714789\n",
      " 0.99818946 0.99823158 0.99823158 0.98600228 0.99891294 0.99818814\n",
      " 0.99823158 0.99823158 0.99297467 0.99823158 0.99823158 0.99855021\n",
      " 0.99823158 0.9976658  0.99518036 0.99782714 0.99863035 0.83714789\n",
      " 0.99851008 0.99818946 0.99823158 0.99823158 0.99822963 0.99871096\n",
      " 0.99459009 0.99823158 0.99823158 0.99798463 0.99823158 0.99738373\n",
      " 0.83714789 0.99827181 0.99806922 0.99823158 0.99891214 0.99726215\n",
      " 0.99818946 0.99851008 0.99851008 0.99827181 0.99810908 0.83714789\n",
      " 0.99826891 0.99522884 0.9987925  0.99581944 0.99919462 0.93220934\n",
      " 0.99823158 0.99722183 0.99823158 0.99823158 0.83714789 0.98600228\n",
      " 0.83714789 0.99823158 0.99823158 0.99823158 0.99522884 0.99826891\n",
      " 0.99899413 0.83714789 0.99722201 0.98600228 0.99823158 0.99855012\n",
      " 0.99823158 0.99806429 0.99823158 0.9973432  0.99823158 0.83714789\n",
      " 0.99823158 0.99826891 0.99590622 0.998712   0.99823158 0.98908872\n",
      " 0.99325116 0.99826891 0.99823158 0.99333566 0.99738373 0.99851008\n",
      " 0.99810908 0.99823158 0.99826891 0.99823158 0.83714789 0.99823158\n",
      " 0.99774656 0.92801481 0.99823158 0.99486822 0.99823158 0.99823158\n",
      " 0.99823158 0.83714789 0.99786743 0.99349742 0.99702021 0.99867066\n",
      " 0.83714789 0.9922519  0.99823158 0.99362536 0.99506463 0.99794818\n",
      " 0.99823158 0.98600228 0.99911427 0.99823158 0.9987925  0.99927565\n",
      " 0.83714789 0.99702018 0.93220934 0.99843012 0.9987925  0.99818946\n",
      " 0.99903388 0.99373917 0.92801481 0.99823158 0.98945728 0.99581944\n",
      " 0.83714789 0.99823158 0.98610351 0.99754495 0.9966972  0.99851008\n",
      " 0.93220934        nan 0.99823158 0.93220934 0.92801481 0.99823158\n",
      " 0.83714789 0.92801481 0.9966972  0.83714789 0.99823158 0.92801481\n",
      " 0.98600228 0.99510071 0.99830885 0.99855012 0.99722183 0.99738353\n",
      " 0.99903359 0.99823158 0.99722201 0.99823158 0.99726215 0.92801481\n",
      " 0.99802837 0.99823158 0.99549905 0.99911427 0.99823158 0.9941858\n",
      " 0.99851008 0.93220934 0.9947812  0.99786743 0.98600228 0.99810908\n",
      " 0.99823158 0.83714789 0.98600228 0.92801481 0.99447028 0.92801481\n",
      " 0.9987925  0.99706055 0.99823158 0.99823158 0.99863092 0.99823158\n",
      " 0.92801481 0.99851008 0.99823158 0.92801481 0.98600228 0.99879222\n",
      " 0.99823158 0.99818946 0.99823132 0.98600228 0.99823158 0.99823158\n",
      " 0.99859081 0.99810908 0.92801481 0.99497984 0.99823158 0.99823158\n",
      " 0.99823158 0.99317692 0.99810908 0.98908872 0.92801481 0.98908872\n",
      "        nan 0.99823158 0.99823158 0.99851008 0.98600228 0.99823158\n",
      " 0.99911449 0.99802837 0.9947812  0.99842938 0.99850934 0.99702021\n",
      " 0.99826891 0.92801481 0.99823158 0.99522884 0.99834854 0.83714789\n",
      " 0.99823158 0.99838841 0.99545864 0.99823158 0.99561895 0.99823158\n",
      " 0.9976658  0.92801481 0.99826891 0.99907394 0.98593933 0.99823158\n",
      " 0.98600228 0.99838952 0.99823158 0.99366615 0.99823158        nan\n",
      " 0.99466918 0.99895369 0.99823158 0.99826891 0.99834798 0.99823158\n",
      " 0.99823158 0.99114773 0.99827181 0.99895341 0.99466918 0.99317692\n",
      "        nan 0.92801481 0.99851008 0.99823158 0.99823158 0.99806922\n",
      " 0.99823158 0.99823158 0.99823158 0.99911427 0.99827155 0.99510071\n",
      " 0.83714789 0.99834798 0.99823158 0.99851008 0.99806857 0.99823158\n",
      " 0.99194952 0.83714789 0.99810727 0.99823158 0.98939383 0.99823158\n",
      " 0.98980069 0.99297467 0.99806857 0.99827181 0.9976658  0.98600228\n",
      " 0.99843012 0.9969798  0.9975852  0.83714789 0.99814983 0.99895369\n",
      " 0.99859081 0.99823158 0.99823158 0.99814931 0.99482105 0.99738373\n",
      " 0.99778687 0.98908872 0.98582856 0.99362536 0.99823132 0.93220934\n",
      " 0.99786743 0.99823158 0.9947812  0.99518036 0.99823158 0.99702021\n",
      " 0.92801481 0.99907437 0.99823158 0.99035197 0.9976658         nan\n",
      " 0.99823158 0.98602186 0.99823158 0.99794818 0.99823158 0.98955617\n",
      " 0.83714789 0.99726215 0.99907437 0.99823132 0.99818814 0.99561895\n",
      " 0.92801481 0.9976658  0.9989934  0.9987925  0.99834809 0.9933826\n",
      " 0.99826875 0.99822876 0.99823158 0.98602186 0.99867066 0.99810908\n",
      " 0.99810908 0.9976658  0.99823158 0.99822876 0.99823158 0.99297467\n",
      " 0.99506463 0.99823158 0.99823158 0.99823158 0.99855021 0.99806922\n",
      " 0.99823158 0.99810908 0.99518036 0.99823158 0.99851008 0.83714789\n",
      " 0.99297467 0.98600228 0.83714789 0.83714789 0.9984292  0.99826891\n",
      " 0.99823158 0.99867066 0.99827181 0.99823158 0.83714789 0.92801481\n",
      " 0.99823158        nan 0.99366111 0.99823158 0.99823132 0.98908872\n",
      " 0.93220934 0.98600228 0.99486822 0.98600228 0.98600228 0.92801481\n",
      " 0.99242839 0.99590622 0.98600228 0.99823158 0.99678203 0.9981096\n",
      " 0.99806857 0.99823158]\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the train scores are non-finite: [0.99836181 0.83714512 0.99836181 0.99830035 0.98624749 0.99836181\n",
      " 0.98694475 0.99836181 0.99864125 0.99833064 0.99836181 0.99868078\n",
      " 0.92801772 0.99836181 0.99782664 0.9992754  0.99836181 0.99703042\n",
      " 0.92801772 0.99910449 0.93221123 0.99897353 0.9992754  0.99853054\n",
      " 0.99751413 0.99715138 0.98921977 0.99844015 0.99873197 0.99822982\n",
      " 0.99802823 0.99836181 0.99836181 0.99853088 0.9992754  0.99167618\n",
      " 0.99853054 0.99477292 0.98921977 0.99654081 0.99836181 0.99778634\n",
      " 0.98652306 0.99647051 0.9943265  0.98921977 0.99893273 0.99255837\n",
      " 0.99931563 0.99836181 0.99820965 0.9997484  0.83714512 0.99574403\n",
      " 0.99844164 0.99847153 0.997625   0.99914393 0.99840196 0.92801772\n",
      " 0.93221123 0.99868165 0.83714512 0.98624749 0.99836181 0.99836181\n",
      " 0.93221123 0.99841203 0.98624749 0.99665149 0.99836181 0.83714512\n",
      " 0.99854031 0.99481272 0.99900376 0.997867   0.99840196 0.99836181\n",
      " 0.99518228 0.99836181 0.99927543 0.99836181 0.99836181 0.93221123\n",
      " 0.93221123 0.98921977 0.99524661 0.99898316 0.99962753 0.99853088\n",
      " 0.99885267 0.9992754  0.99836181 0.99838021 0.99912457 0.99836181\n",
      " 0.93221123 0.99836181 0.99488234 0.99413288 0.98937864 0.99869173\n",
      " 0.99836181 0.99917419 0.99836181 0.99962751 0.99390301 0.99731249\n",
      " 0.99836181 0.83714512 0.99836181 0.99605162 0.99918422 0.83714512\n",
      " 0.99872191 0.99836181 0.99836181 0.98624749 0.99926511 0.99865065\n",
      " 0.99836181 0.99836181 0.9932589  0.99836181 0.99836181 0.99903327\n",
      " 0.99836181 0.99799802 0.99528098 0.99817942 0.99914393 0.83714512\n",
      " 0.99900376 0.99872191 0.99836181 0.99836181 0.99873197 0.99917419\n",
      " 0.99531347 0.99836181 0.99836181 0.99854997 0.99836181 0.99778634\n",
      " 0.83714512 0.99841203 0.99838021 0.99836181 0.99937561 0.99745359\n",
      " 0.99872191 0.99902391 0.99902391 0.99841203 0.99853088 0.83714512\n",
      " 0.99853054 0.99518228 0.99920498 0.99685242 0.9996678  0.93221123\n",
      " 0.99836181 0.99751413 0.99836181 0.99836181 0.83714512 0.98624749\n",
      " 0.83714512 0.99836181 0.99836181 0.99836181 0.99518228 0.99853054\n",
      " 0.9995773  0.83714512 0.99757456 0.98624749 0.99836181 0.99903337\n",
      " 0.99836181 0.99878178 0.99836181 0.99769558 0.99836181 0.83714512\n",
      " 0.99836181 0.99853054 0.99620761 0.99918495 0.99836181 0.98921977\n",
      " 0.99353439 0.99853054 0.99836181 0.99354847 0.99778634 0.99902391\n",
      " 0.99851072 0.99836181 0.99853054 0.99836181 0.83714512 0.99836181\n",
      " 0.99809879 0.92801772 0.99836181 0.99574403 0.99836181 0.99836181\n",
      " 0.99836181 0.83714512 0.99822982 0.99442577 0.99731249 0.99916409\n",
      " 0.83714512 0.99314448 0.99836181 0.99388062 0.99524661 0.99830035\n",
      " 0.99836181 0.98624749 0.99941621 0.99836181 0.99920498 0.99950687\n",
      " 0.83714512 0.99751413 0.93221123 0.99906403 0.99922513 0.99872191\n",
      " 0.99955709 0.9938288  0.92801772 0.99836181 0.98962304 0.99685242\n",
      " 0.83714512 0.99836181 0.98657357 0.99782664 0.99715138 0.99900376\n",
      " 0.93221123        nan 0.99836181 0.93221123 0.92801772 0.99836181\n",
      " 0.83714512 0.92801772 0.99715138 0.83714512 0.99836181 0.92801772\n",
      " 0.98624749 0.99633975 0.99892275 0.99903327 0.99751413 0.99772581\n",
      " 0.99937593 0.99836181 0.99757456 0.99836181 0.99745359 0.92801772\n",
      " 0.99844015 0.99836181 0.99647051 0.99941621 0.99836181 0.99423513\n",
      " 0.99902391 0.93221123 0.99481272 0.99820965 0.98624749 0.99853088\n",
      " 0.99836181 0.83714512 0.98624749 0.92801772 0.99465132 0.92801772\n",
      " 0.99922513 0.99739316 0.99836181 0.99836181 0.99917438 0.99836181\n",
      " 0.92801772 0.99897353 0.99836181 0.92801772 0.98624749 0.99927543\n",
      " 0.99836181 0.99872191 0.99836115 0.98624749 0.99836181 0.99836181\n",
      " 0.99912457 0.99853088 0.92801772 0.99622944 0.99836181 0.99836181\n",
      " 0.99836181 0.99390301 0.99851072 0.98921977 0.92801772 0.98921977\n",
      "        nan 0.99836181 0.99836181 0.99900376 0.98624749 0.99836181\n",
      " 0.9994263  0.9984704  0.99479282 0.99904339 0.99905365 0.99731249\n",
      " 0.99853054 0.92801772 0.99836181 0.99518228 0.99876108 0.83714512\n",
      " 0.99836181 0.99885267 0.99647051 0.99836181 0.99665149 0.99836181\n",
      " 0.99799802 0.92801772 0.99853054 0.99937593 0.98641844 0.99836181\n",
      " 0.98624749 0.99875176 0.99836181 0.99409804 0.99836181        nan\n",
      " 0.99562372 0.9992754  0.99836181 0.99853054 0.99875182 0.99836181\n",
      " 0.99836181 0.99147265 0.99841203 0.9993559  0.99562372 0.99390301\n",
      "        nan 0.92801772 0.99897353 0.99836181 0.99836181 0.99838021\n",
      " 0.99836181 0.99836181 0.99836181 0.99941621 0.99842149 0.99633975\n",
      " 0.83714512 0.99875182 0.99836181 0.99902391 0.99858094 0.99836181\n",
      " 0.9921887  0.83714512 0.99856999 0.99836181 0.98979239 0.99836181\n",
      " 0.99019732 0.9932589  0.99861118 0.99840196 0.99799802 0.98624749\n",
      " 0.99900364 0.99739316 0.997867   0.83714512 0.99853104 0.9992754\n",
      " 0.99912457 0.99836181 0.99836181 0.99868165 0.99488234 0.99778634\n",
      " 0.99813907 0.98921977 0.98625334 0.99388062 0.99837124 0.93221123\n",
      " 0.99824996 0.99836181 0.99479282 0.99528098 0.99836181 0.99731249\n",
      " 0.92801772 0.99931563 0.99836181 0.99051894 0.99802823        nan\n",
      " 0.99836181 0.98640063 0.99836181 0.99830035 0.99836181 0.99001484\n",
      " 0.83714512 0.99745359 0.99932564 0.99837124 0.99870106 0.99665149\n",
      " 0.92801772 0.99799802 0.99948664 0.99920498 0.99890291 0.99393701\n",
      " 0.99875157 0.99864125 0.99836181 0.98640063 0.99916409 0.99853088\n",
      " 0.99851072 0.99799802 0.99836181 0.99866134 0.99836181 0.9932589\n",
      " 0.99524661 0.99836181 0.99836181 0.99836181 0.99903327 0.99838021\n",
      " 0.99836181 0.9985208  0.99527103 0.99836181 0.99900376 0.83714512\n",
      " 0.9932589  0.98624749 0.83714512 0.83714512 0.99876108 0.99853054\n",
      " 0.99836181 0.99911382 0.99841203 0.99836181 0.83714512 0.92801772\n",
      " 0.99836181        nan 0.99449726 0.99836181 0.99836115 0.98921977\n",
      " 0.93221123 0.98624749 0.99574403 0.98624749 0.98624749 0.92801772\n",
      " 0.99255837 0.99625763 0.98624749 0.99836181 0.99788878 0.99841046\n",
      " 0.99858094 0.99836181]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.9993156117159033\n",
      "... with parameters: {'min_samples_split': 2, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.0001, 'max_leaf_nodes': 95, 'max_depth': 18, 'criterion': 'gini'}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,100),  \n",
    "    'min_samples_leaf': np.arange(1,100),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 100), \n",
    "    'max_depth': np.arange(1,25), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29c8ed75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1024 candidates, totalling 5120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "3200 fits failed out of a total of 5120.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1280 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 889, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 177, in fit\n",
      "    self._validate_params()\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py\", line 600, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_leaf' parameter of DecisionTreeClassifier must be an int in the range [1, inf) or a float in the range (0.0, 1.0). Got 0 instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "960 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 889, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 177, in fit\n",
      "    self._validate_params()\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py\", line 600, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_split' parameter of DecisionTreeClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 0 instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "960 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 889, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 177, in fit\n",
      "    self._validate_params()\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py\", line 600, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_split' parameter of DecisionTreeClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan ...        nan 0.99919537 0.99919537]\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the train scores are non-finite: [       nan        nan        nan ...        nan 0.99955727 0.99955727]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.9995973910381597\n",
      "... with parameters: {'criterion': 'gini', 'max_depth': 18, 'max_leaf_nodes': 94, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 5\n",
    "min_samples_split = rand_search.best_params_['min_samples_split']\n",
    "min_samples_leaf = rand_search.best_params_['min_samples_leaf']\n",
    "min_impurity_decrease = rand_search.best_params_['min_impurity_decrease']\n",
    "max_leaf_nodes = rand_search.best_params_['max_leaf_nodes']\n",
    "max_depth = rand_search.best_params_['max_depth']\n",
    "criterion = rand_search.best_params_['criterion']\n",
    "#Using the best parameters from the Random Search to use as range for the parameters to do the grid search\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(min_samples_split-2,min_samples_split+2),  \n",
    "    'min_samples_leaf': np.arange(min_samples_leaf-2,min_samples_leaf+2),\n",
    "    'min_impurity_decrease': np.arange(min_impurity_decrease-0.0001, min_impurity_decrease+0.0001, 0.00005),\n",
    "    'max_leaf_nodes': np.arange(max_leaf_nodes-2,max_leaf_nodes+2), \n",
    "    'max_depth': np.arange(max_depth-2,max_depth+2), \n",
    "    'criterion': [criterion]\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = dtree, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d78e1c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.9997871 Precision=0.9998500 Recall=0.9998500 F1=0.9998500\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")\n",
    "F1_Decisiontree=2*TP/(2*TP+FP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27905dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 score from the Decision Tree using Random Search and Grid Search is 0.9998500299940012\n"
     ]
    }
   ],
   "source": [
    "print(f\"The f1 score from the Decision Tree using Random Search and Grid Search is {F1_Decisiontree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "042d0315",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Decision Tree\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb59bed",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "#### Without Randomsearch and gridsearch cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "243389d5",
   "metadata": {
    "id": "5WfGTWb3hYd-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 38.7 s\n",
      "Wall time: 17.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ann = MLPClassifier(hidden_layer_sizes=(60,50,40), solver='adam', max_iter=200)\n",
    "_ = ann.fit(X_train,np.ravel( y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a421f866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 62.5 ms\n",
      "Wall time: 42 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = ann.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "754ac0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5453\n",
      "           1       1.00      1.00      1.00     13336\n",
      "\n",
      "    accuracy                           1.00     18789\n",
      "   macro avg       1.00      1.00      1.00     18789\n",
      "weighted avg       1.00      1.00      1.00     18789\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1695e92e",
   "metadata": {},
   "source": [
    "## With RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb384edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "1 fits failed out of a total of 500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 749, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 491, in _fit\n",
      "    raise ValueError(\n",
      "ValueError: Solver produced non-finite parameter weights. The input data may contain large values and need to be preprocessed.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.99313637 0.99895327 0.92044481 0.86546005 0.98130873 0.3769805\n",
      " 0.83127121 0.97504646 0.96348177 0.9969774  0.4        0.95364692\n",
      " 0.53333333 0.92696937 0.95606758 0.9365016  0.97959486 0.94559101\n",
      " 0.93809187 0.98727207 0.91426837 0.95967204 0.95056072 0.94972826\n",
      " 0.96867711 0.90232546 0.87837566 0.90374253 0.95423156 0.98374951\n",
      " 0.98408528 0.95946735 0.90859796 0.91263188 0.97514655 0.93726681\n",
      " 0.88146117 0.91625027 0.13333333 0.99370071 0.99291856 0.94372714\n",
      " 0.40348086 0.94557198 0.94599744 0.98499025 0.94454479 0.99324048\n",
      " 0.93817643 0.99332685 0.95391601 0.80018855 0.92384921 0.98208499\n",
      "        nan 0.99891401 0.96547618 0.9412494  0.99831025 0.99156891\n",
      " 0.90710438 0.98407264 0.99879324 0.98843815 0.91337853 0.5071931\n",
      " 0.9106141  0.8128939  0.53333333 0.95319575 0.98244663 0.99465966\n",
      " 0.90334909 0.97681471 0.95076718 0.4217479  0.97847217 0.9899381\n",
      " 0.685105   0.99839057 0.92487019 0.96767255 0.97525666 0.99768634\n",
      " 0.99923521 0.97166042 0.89251306 0.92128401 0.99169747 0.93161707\n",
      " 0.94316795 0.94231111 0.9965833  0.98007005 0.93906176 0.91020784\n",
      " 0.13333333 0.9589984  0.98313397 0.95773466]\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the train scores are non-finite: [0.99322517 0.99927533 0.92070038 0.86603475 0.98224645 0.37558795\n",
      " 0.83071474 0.97433944 0.96433001 0.99777334 0.4        0.95386197\n",
      " 0.53333333 0.9263572  0.95608507 0.93406543 0.97973938 0.94674139\n",
      " 0.93799503 0.98716353 0.91395935 0.9602586  0.94942549 0.95003758\n",
      " 0.97074588 0.90209766 0.87867105 0.90398422 0.95468836 0.9845039\n",
      " 0.9836941  0.96002332 0.90875887 0.91146594 0.97573669 0.9385234\n",
      " 0.88169634 0.91699656 0.13333333 0.99420108 0.99320114 0.94373685\n",
      " 0.40325369 0.94376861 0.9454483  0.98508281 0.94572895 0.99359341\n",
      " 0.93967096 0.99339683 0.95370492 0.798387   0.92477668 0.98350682\n",
      "        nan 0.99941649 0.963734   0.94143384 0.99862236 0.9918515\n",
      " 0.90655789 0.98472806 0.99920551 0.9890173  0.91158972 0.5051612\n",
      " 0.91163219 0.8127689  0.53333333 0.95334031 0.98168756 0.99499016\n",
      " 0.9033719  0.97686411 0.95191596 0.41793287 0.97778773 0.99125133\n",
      " 0.68514624 0.99902423 0.92664275 0.96691636 0.97572166 0.99758161\n",
      " 0.99967803 0.97012167 0.89283304 0.92143747 0.99096091 0.93189522\n",
      " 0.94331774 0.94182063 0.99716639 0.9798838  0.93912476 0.91163213\n",
      " 0.13333333 0.9593005  0.98419614 0.95702179]\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'sgd', 'max_iter': 3000, 'learning_rate_init': 0.01, 'learning_rate': 'adaptive', 'hidden_layer_sizes': (70, 50, 40), 'alpha': 0, 'activation': 'relu'}\n",
      "CPU times: total: 1min 9s\n",
      "Wall time: 24min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "score_measure = \"f1\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [ (50,), (70,),(50,30), (40,20),(60,60,60), (60,70, 80), (70,50,40)],\n",
    "    'activation': ['logistic', 'tanh', 'relu'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0, .2, .5, .7, 1],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'learning_rate_init': [0.001, 0.01, 0.1, 0.2, 0.5],\n",
    "    'max_iter': [3000]\n",
    "}\n",
    "\n",
    "ann = MLPClassifier()\n",
    "grid_search = RandomizedSearchCV(estimator = ann, param_distributions=param_grid, cv=kfolds, n_iter=100,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_\n",
    "\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b0a558f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5453\n",
      "           1       1.00      1.00      1.00     13336\n",
      "\n",
      "    accuracy                           1.00     18789\n",
      "   macro avg       1.00      1.00      1.00     18789\n",
      "weighted avg       1.00      1.00      1.00     18789\n",
      "\n",
      "CPU times: total: 109 ms\n",
      "Wall time: 59.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = bestRecallTree.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819af11d",
   "metadata": {},
   "source": [
    "## With GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d015048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajayk\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'alpha': 0, 'hidden_layer_sizes': (50, 90), 'learning_rate': 'adaptive', 'learning_rate_init': 0.25, 'max_iter': 5000, 'solver': 'sgd'}\n",
      "CPU times: total: 1min 35s\n",
      "Wall time: 26min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "score_measure = \"f1\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [ (50,30), (50,70), (50,90)],\n",
    "    'activation': ['logistic', 'relu'],\n",
    "    'solver': ['sgd'],\n",
    "    'alpha': [0,.5 ],\n",
    "    'learning_rate': ['adaptive', 'invscaling'],\n",
    "    'learning_rate_init': [0.1,0.2,0.25],\n",
    "    'max_iter': [5000]\n",
    "}\n",
    "\n",
    "ann = MLPClassifier()\n",
    "grid_search = GridSearchCV(estimator = ann, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_\n",
    "\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54a95689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.99      0.88      5453\n",
      "           1       0.99      0.90      0.94     13336\n",
      "\n",
      "    accuracy                           0.92     18789\n",
      "   macro avg       0.89      0.94      0.91     18789\n",
      "weighted avg       0.94      0.92      0.92     18789\n",
      "\n",
      "CPU times: total: 344 ms\n",
      "Wall time: 148 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = bestRecallTree.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1dd4320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.9225611 Precision=0.9938482 Recall=0.8964457 F1=0.9426375\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, y_pred)\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")\n",
    "F1_lr=2*TP/(2*TP+FP+FN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f2887cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score from the Neural Networks model using Random Search and Grid Search is :0.9426374926079243\n"
     ]
    }
   ],
   "source": [
    "print(f\"The F1 score from the Neural Networks model using Random Search and Grid Search is :{F1_lr}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ecb22c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.concat([performance, pd.DataFrame({'model':\"neural network using random & grid search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d59e5fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "260e3bdd",
   "metadata": {},
   "source": [
    "## Keras with SKlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e5f1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "918dbb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# If you don't have the following installed, from command line '!pip install scikeras'\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras.initializers import GlorotNormal\n",
    "\n",
    "score_measure = \"f1\"\n",
    "kfolds = 5\n",
    "\n",
    "def build_clf(hidden_layer_sizes, dropout):\n",
    "    ann = tf.keras.models.Sequential()\n",
    "    ann.add(keras.layers.Input(shape=12)),\n",
    "    for hidden_layer_size in hidden_layer_sizes:\n",
    "        ann.add(keras.layers.Dense(hidden_layer_size, kernel_initializer= tf.keras.initializers.GlorotUniform(), \n",
    "                                     bias_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None), activation=\"relu\"))\n",
    "        ann.add(keras.layers.Dropout(dropout))\n",
    "    ann.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    #the out is either 0 or 1 which is single numeric value, like a regression task so the tensor shape is (None,1)\n",
    "    \n",
    "    ann.compile(loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return ann\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dfbfd8",
   "metadata": {},
   "source": [
    "For more information on dense layers and initializers, see the following:\n",
    "* https://keras.io/api/layers/core_layers/dense/\n",
    "* https://keras.io/api/layers/initializers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6992c66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "keras_clf = KerasClassifier(\n",
    "    model=build_clf,\n",
    "    hidden_layer_sizes=64,\n",
    "    dropout = 0.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7501769a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'build_fn', 'warm_start', 'random_state', 'optimizer', 'loss', 'metrics', 'batch_size', 'validation_batch_size', 'verbose', 'callbacks', 'validation_split', 'shuffle', 'run_eagerly', 'epochs', 'hidden_layer_sizes', 'dropout', 'class_weight'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "params = {\n",
    "    'optimizer__learning_rate': [0.0005, 0.001, 0.005],\n",
    "    'model__hidden_layer_sizes': [(70,),(90, ), (100,), (100, 90)],\n",
    "    'model__dropout': [0, 0.1],\n",
    "    'batch_size':[20, 60, 100],\n",
    "    'epochs':[10, 50, 100],\n",
    "    'optimizer':[\"adam\",'sgd']\n",
    "}\n",
    "keras_clf.get_params().keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "de41dde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 3ms/step\n",
      "83/83 [==============================] - 0s 3ms/step\n",
      "83/83 [==============================] - 0s 3ms/step\n",
      "83/83 [==============================] - 0s 3ms/step\n",
      "83/83 [==============================] - 0s 3ms/step\n",
      "50/50 [==============================] - 0s 3ms/step\n",
      "50/50 [==============================] - 0s 3ms/step\n",
      "50/50 [==============================] - 0s 3ms/step\n",
      "50/50 [==============================] - 0s 3ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 3ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 3ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 3ms/step\n",
      "50/50 [==============================] - 0s 3ms/step\n",
      "50/50 [==============================] - 0s 3ms/step\n",
      "50/50 [==============================] - 0s 3ms/step\n",
      "50/50 [==============================] - 0s 4ms/step\n",
      "50/50 [==============================] - 0s 4ms/step\n",
      "50/50 [==============================] - 0s 5ms/step\n",
      "50/50 [==============================] - 0s 4ms/step\n",
      "50/50 [==============================] - 0s 3ms/step\n",
      "50/50 [==============================] - 0s 3ms/step\n",
      "50/50 [==============================] - 0s 3ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 3ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 3ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 3ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "249/249 [==============================] - 1s 3ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "83/83 [==============================] - 1s 7ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 3ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 3ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 3ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 0s 1ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 3ms/step\n",
      "83/83 [==============================] - 0s 3ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 3ms/step\n",
      "50/50 [==============================] - 0s 3ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 3ms/step\n",
      "50/50 [==============================] - 0s 3ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 3ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 0s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 1ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "249/249 [==============================] - 0s 1ms/step\n",
      "249/249 [==============================] - 0s 1ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 1ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 1ms/step\n",
      "83/83 [==============================] - 0s 1ms/step\n",
      "83/83 [==============================] - 0s 1ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 0s 2ms/step\n",
      "249/249 [==============================] - 0s 1ms/step\n",
      "249/249 [==============================] - 0s 1ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 1ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 1ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 0s 2ms/step\n",
      "249/249 [==============================] - 0s 1ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 0s 1ms/step\n",
      "249/249 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 3ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n",
      "50/50 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "rnd_search_cv = RandomizedSearchCV(estimator=keras_clf, param_distributions=params, scoring='f1', n_iter=50, cv=5)\n",
    "\n",
    "import sys\n",
    "sys.setrecursionlimit(10000) # note: the default is 3000 (python 3.9)\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')\n",
    "callback = [earlystop]\n",
    "\n",
    "_ = rnd_search_cv.fit(X_train, y_train, callbacks=callback, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "942cc8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'optimizer__learning_rate': 0.0005,\n",
       " 'optimizer': 'sgd',\n",
       " 'model__hidden_layer_sizes': (100, 90),\n",
       " 'model__dropout': 0.1,\n",
       " 'epochs': 100,\n",
       " 'batch_size': 100}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eb3c1968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer__learning_rate': 0.0005, 'optimizer': 'sgd', 'model__hidden_layer_sizes': (100, 90), 'model__dropout': 0.1, 'epochs': 100, 'batch_size': 100}\n"
     ]
    }
   ],
   "source": [
    "best_net = rnd_search_cv.best_estimator_\n",
    "print(rnd_search_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2ac8d5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 1s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5453\n",
      "           1       1.00      1.00      1.00     13336\n",
      "\n",
      "    accuracy                           1.00     18789\n",
      "   macro avg       1.00      1.00      1.00     18789\n",
      "weighted avg       1.00      1.00      1.00     18789\n",
      "\n",
      "CPU times: total: 3.84 s\n",
      "Wall time: 1.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = best_net.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f4d141b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, best_net.predict(X_test))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9b8a37",
   "metadata": {},
   "source": [
    "Here we can clreally observe that the model is predicting the output soo good that we have very less false postives and flase negatives in the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a029babb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.9997339 Precision=0.9998500 Recall=0.9997750 F1=0.9998125\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, y_pred)\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")\n",
    "F1_lr=2*TP/(2*TP+FP+FN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fe0e4b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score from the Deep Neural Networks model using keras is :0.9998125304637996\n"
     ]
    }
   ],
   "source": [
    "print(f\"The F1 score from the Deep Neural Networks model using keras is :{F1_lr}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "90f75a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.concat([performance, pd.DataFrame({'model':\"deep neural network using keras with sklearn\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a550d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eac50f49",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0d82fc99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic using random &amp; grid search</td>\n",
       "      <td>0.905264</td>\n",
       "      <td>0.963873</td>\n",
       "      <td>0.900270</td>\n",
       "      <td>0.930986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm using Random &amp; Grid search</td>\n",
       "      <td>0.999521</td>\n",
       "      <td>0.999700</td>\n",
       "      <td>0.999625</td>\n",
       "      <td>0.999663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.999787</td>\n",
       "      <td>0.999850</td>\n",
       "      <td>0.999850</td>\n",
       "      <td>0.999850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neural network using random &amp; grid search</td>\n",
       "      <td>0.922561</td>\n",
       "      <td>0.993848</td>\n",
       "      <td>0.896446</td>\n",
       "      <td>0.942637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>deep neural network using keras with sklearn</td>\n",
       "      <td>0.999734</td>\n",
       "      <td>0.999850</td>\n",
       "      <td>0.999775</td>\n",
       "      <td>0.999813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          model  Accuracy  Precision  \\\n",
       "0           logistic using random & grid search  0.905264   0.963873   \n",
       "0                svm using Random & Grid search  0.999521   0.999700   \n",
       "0                                 Decision Tree  0.999787   0.999850   \n",
       "0     neural network using random & grid search  0.922561   0.993848   \n",
       "0  deep neural network using keras with sklearn  0.999734   0.999850   \n",
       "\n",
       "     Recall        F1  \n",
       "0  0.900270  0.930986  \n",
       "0  0.999625  0.999663  \n",
       "0  0.999850  0.999850  \n",
       "0  0.896446  0.942637  \n",
       "0  0.999775  0.999813  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5120c6f1",
   "metadata": {},
   "source": [
    "We can clearlly observ that the F1 score of the Decision Tree and deep neural networks uisng keras are much higher when comapred to other models. But in general, DNN's using Keras will provide higher accuracy than decision trees for complex and high-dimensional data.Even though the decision trees are faster and more interpretable than DNN's as our data is large and more complex so lets consider the DNN's as the best model.\n",
    "\n",
    "So, the best model that has less False Positives(False Alarms) and False Negatives(Smoke is there but Detected that there is no smoke) is **Deep Neural Networks using keras**. \n",
    "\n",
    "Now that we have our best model we can deploye this into an AI based smoke detector and it can predict the smoke with less False Negatives and False Postives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db403e30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a239d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
